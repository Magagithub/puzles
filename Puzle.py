"""
ðŸ§© 1ï¸âƒ£ RepresentaciÃ³n del puzzle: el Estado
goal_state = (0, 1, 2, 3, 4, 5, 6, 7, 8)


ðŸ“Œ Un estado es una tupla de 9 elementos.

NÃºmero	Significa
0â€“7	Piezas del puzzle
8	El hueco en blanco

Ejemplo de estado:

(2, 5, 1,
 3, 8, 4,
 6, 7, 0)


El agente SOLO ve esto.
La imagen solo se usa para dibujar en la interfaz.

ðŸ” 2ï¸âƒ£ Acciones y movimiento en la cuadrÃ­cula
actions = ["up", "right", "down", "left"]
moves = {
    "up": -3,
    "right": 1,
    "down": 3,
    "left": -1
}


ðŸ“Œ El hueco (8) se mueve segÃºn:

AcciÃ³n	Movimiento	Ejemplo (Ã­ndice hueco)
up	-3 posiciones	7 â†’ 4
right	+1	4 â†’ 5
down	+3	3 â†’ 6
left	-1	5 â†’ 4
ðŸ§  3ï¸âƒ£ QuÃ© acciones son vÃ¡lidas
def valid_actions(state):


ðŸ”¹ No puedes mover fuera del tablero.
Ejemplo: si la ficha estÃ¡ en la fila 0 â†’ no hay up

ðŸ“ 4ï¸âƒ£ HeurÃ­stica: distancia Manhattan
def manhattan_distance(state):


Cuenta cuÃ¡ntos movimientos le falta a cada pieza para llegar a la posiciÃ³n correcta:

mientras mÃ¡s pequeÃ±a â†’ mÃ¡s cerca del objetivo â†’ mejor

Ejemplo:

Si el 1 estÃ¡ 2 posiciones lejos â†’ suma +2.

ðŸŽ¯ Esta heurÃ­stica es lo que hace que aprenda mucho mejor.

ðŸ† 5ï¸âƒ£ Recompensas: step_env()
def step_env(state, action):


Â¿QuÃ© hace?

1ï¸âƒ£ Comprueba si la acciÃ³n es vÃ¡lida
2ï¸âƒ£ Calcula nueva posiciÃ³n
3ï¸âƒ£ Calcula recompensa (reward)

ðŸ“Œ Reglas de recompensa:

SituaciÃ³n	Reward
AcciÃ³n invÃ¡lida	-5
Movimiento normal	-1
Si mejora distancia Manhattan	+ (distancia_antigua âˆ’ distancia_nueva)
Si llega al objetivo	+50 extra

âž¡ï¸ Da puntos por acercarse al objetivo
âž¡ï¸ Penaliza alejarse o dar vueltas sin sentido

Esto se llama reward shaping.

ðŸ”€ 6ï¸âƒ£ Barajar el puzzle
def shuffle_state(state, moves_count=20)


Aplica movimientos aleatorios vÃ¡lidos para mezclarlo.

ðŸ“Œ No es un mezclado al azar total, asÃ­ que siempre se puede resolver.

ðŸ“š 7ï¸âƒ£ Tabla Q
Q = defaultdict(lambda: {a: 0.0 for a in actions})


ðŸ“Œ Q es un diccionario de diccionarios:

Q[estado][accion] = valor


Ejemplo:

Q[(0,1,2,3,4,5,6,7,8)]["right"] = 4.52


Cuanto mayor sea el valor â†’ mejor acciÃ³n.

ðŸ§© 8ï¸âƒ£ PolÃ­tica epsilon-greedy
def epsilon_greedy(state, epsilon):


ðŸ“Œ Decide si elige:

Tipo	CuÃ¡ndo	Para quÃ©
AcciÃ³n aleatoria	prob. epsilon	Explorar
Mejor acciÃ³n segÃºn Q	prob. 1-epsilon	Explotar conocimiento

ðŸ”¹ Al inicio eps = 1 â†’ CASI TODO ES EXPLORAR
ðŸ”¹ Luego baja â†’ se vuelve mÃ¡s inteligente

ðŸ“ˆ 9ï¸âƒ£ ActualizaciÃ³n Q-Learning
def q_update(s, a, r, s2, alpha, gamma):


FÃ³rmula oficial:

Q(s,a) â† Q(s,a) + Î± * (r + Î³ * max_a' Q(s2, a') - Q(s,a))


ðŸ“Œ ParÃ¡metros

Letra	QuÃ© es	Valor recomendado
Î± (alpha)	Tasa aprendizaje	0.2
Î³ (gamma)	Importancia futuro	0.99
r	recompensa actual	segÃºn reward
max Q	mejor acciÃ³n del futuro	aprendizaje
ðŸ‹ï¸â€â™‚ï¸ ðŸ”Ÿ Entrenamiento Q-Learning
train_q_learning(
    episodes=20000,
    alpha=0.2,
    gamma=0.99,
    eps_start=1.0,
    eps_min=0.05,
    max_steps=200,
    shuffle_depth=20
)


ðŸ“Œ El agente aprende jugando miles de partidas

ParÃ¡metro	Significa
episodes=20000	Partidas de entrenamiento
eps_start=1.0	Al inicio: explorar mucho
eps_min=0.05	Luego: elegir lo mejor
max_steps=200	No se atasca eternamente
shuffle_depth=20	Primero aprende casos simples

ðŸŽ¯ GrÃ¡ficas:

Pasos necesarios por episodio â¬‡ï¸ con el tiempo

Ã‰xitos por episodio â¬†ï¸ con el tiempo

ðŸ¤– 1ï¸âƒ£1ï¸âƒ£ Resolver usando la Q aprendida
solve_with_Q_from_state(...)


ðŸ“Œ Elige casi siempre la mejor acciÃ³n
pero un poquito de exploraciÃ³n (epsilon pequeÃ±o)

ðŸ–¼ï¸ 1ï¸âƒ£2ï¸âƒ£ Interfaz grÃ¡fica Tkinter

Solo es para visualizar:

BotÃ³n cargar imagen â†’ dividir la imagen en 3Ã—3

BotÃ³n barajar â†’ mezclar el puzzle

BotÃ³n entrenar â†’ lanzar Q-learning

BotÃ³n resolver â†’ animaciÃ³n de movimientos

ðŸ“Œ Nada de la imagen se usa en la lÃ³gica del puzzle

ðŸŽ¯ RESUMEN VISUAL
Estado (posiciones)
       â†“
    Q-Learning
       â†“
 Tabla Q aprende mejores movimientos
       â†“
Resolver con Q
       â†“
Tkinter muestra piezas moviÃ©ndose


La IA cree que son nÃºmeros, no trozos de imagen.
"""

# ============================================
# 0. IMPORTACIONES
# ============================================
import tkinter as tk
from tkinter import filedialog
from PIL import Image, ImageTk
import random
import numpy as np
import matplotlib.pyplot as plt
from collections import defaultdict
import threading
import time

# -------------------
# 8-PUZZLE LOGIC
# -------------------

goal_state = (0, 1, 2, 3, 4, 5, 6, 7, 8)
actions = ["up", "right", "down", "left"]

moves = {
    "up": -3,
    "right": 1,
    "down": 3,
    "left": -1
}

# Precomputamos posiciones objetivo de cada ficha para la heurÃ­stica
goal_pos = {tile: idx for idx, tile in enumerate(goal_state)}


def valid_actions(state):
    hole = state.index(8)
    row, col = divmod(hole, 3)
    valid = []
    if row > 0:
        valid.append("up")
    if row < 2:
        valid.append("down")
    if col < 2:
        valid.append("right")
    if col > 0:
        valid.append("left")
    return valid


def manhattan_distance(state):
    """
    HeurÃ­stica: suma de distancias Manhattan de cada ficha a su posiciÃ³n objetivo.
    No contamos el hueco (8).
    """
    dist = 0
    for idx, tile in enumerate(state):
        if tile == 8:
            continue
        r, c = divmod(idx, 3)
        gr, gc = divmod(goal_pos[tile], 3)
        dist += abs(r - gr) + abs(c - gc)
    return dist


def step_env(state, action):
    """
    TransiciÃ³n de entorno + reward shaping.
    - Si la acciÃ³n es invÃ¡lida: penalizaciÃ³n.
    - Si es vÃ¡lida: se mueve el hueco, coste por paso,
      y se aÃ±ade un tÃ©rmino de recompensa segÃºn si nos acercamos o alejamos del objetivo
      usando la distancia Manhattan.
    """
    if action not in valid_actions(state):
        # PenalizaciÃ³n fuerte por acciÃ³n invÃ¡lida
        return state, -5

    old_dist = manhattan_distance(state)

    hole = state.index(8)
    new_pos = hole + moves[action]
    s = list(state)
    s[hole], s[new_pos] = s[new_pos], s[hole]
    new_state = tuple(s)

    new_dist = manhattan_distance(new_state)

    # Coste base por paso
    reward = -1

    # Reward shaping: positivo si mejoramos la distancia, negativo si empeoramos
    reward += (old_dist - new_dist)

    # Bonus fuerte al llegar al objetivo
    if new_state == goal_state:
        reward += 50

    return new_state, reward


def shuffle_state(state, moves_count=20):
    """
    Baraja el puzzle aplicando 'moves_count' movimientos aleatorios vÃ¡lidos.
    """
    s = state
    for _ in range(moves_count):
        a = random.choice(valid_actions(s))
        s, _ = step_env(s, a)
    return s


# Tabla Q: para cada estado, diccionario acciÃ³n->valor
Q = defaultdict(lambda: {a: 0.0 for a in actions})


def epsilon_greedy(state, epsilon):
    va = valid_actions(state)
    if random.random() < epsilon:
        return random.choice(va)
    qv = Q[state]
    return max(va, key=lambda a: qv[a])


def q_update(s, a, r, s2, alpha, gamma):
    best_next = max(Q[s2].values())
    Q[s][a] = Q[s][a] + alpha * (r + gamma * best_next - Q[s][a])


def train_q_learning(
    episodes=20000,
    alpha=0.2,
    gamma=0.99,
    eps_start=1.0,
    eps_min=0.05,
    max_steps=200,
    shuffle_depth=20
):
    """
    Entrena el agente con Q-Learning.
    - MÃ¡s episodios y Ã©psilon decreciente para una exploraciÃ³n-explotaciÃ³n razonable.
    - Barajado no demasiado profundo para que aprenda primero estados alcanzables.
    """
    global Q
    Q = defaultdict(lambda: {a: 0.0 for a in actions})

    steps_log = []
    success_log = []

    epsilon = eps_start

    for ep in range(episodes):
        state = shuffle_state(goal_state, shuffle_depth)
        steps = 0
        success = 0

        for _ in range(max_steps):
            a = epsilon_greedy(state, epsilon)
            s2, r = step_env(state, a)
            q_update(state, a, r, s2, alpha, gamma)

            state = s2
            steps += 1

            if state == goal_state:
                success = 1
                break

        steps_log.append(steps)
        success_log.append(success)

        # Decaimiento del epsilon (no baja de eps_min)
        epsilon = max(eps_min, epsilon * 0.9995)

    return steps_log, success_log


def solve_with_Q_from_state(initial_state, max_steps=200, epsilon_exec=0.05):
    """
    Usa la tabla Q aprendida para resolver desde un estado concreto.
    - PolÃ­tica casi-greedy (epsilon pequeÃ±o) para evitar bucles.
    - Sin visited: dejamos que max_steps ponga el lÃ­mite.
    """
    state = initial_state
    path = [state]

    for _ in range(max_steps):
        if state == goal_state:
            break

        va = valid_actions(state)

        # Un poquito de exploraciÃ³n en ejecuciÃ³n
        if random.random() < epsilon_exec:
            action = random.choice(va)
        else:
            qvals = Q[state]
            action = max(va, key=lambda a: qvals[a])

        new_state, _ = step_env(state, action)
        state = new_state
        path.append(state)

        if state == goal_state:
            break

    return path


# =====================================
# GUI (Tkinter)
# =====================================

class PuzzleApp:
    def __init__(self, root):
        self.root = root
        self.root.title("8-Puzzle con Q-Learning - Miguel MagariÃ±o")

        self.frame = tk.Frame(root)
        self.frame.pack()

        # botones
        tk.Button(self.frame, text="Cargar imagen", command=self.load_image).grid(row=0, column=0)
        tk.Button(self.frame, text="Barajar", command=self.shuffle_puzzle).grid(row=0, column=1)
        tk.Button(self.frame, text="Entrenar Q-Learning", command=self.train_agent).grid(row=0, column=2)
        tk.Button(self.frame, text="Resolver", command=self.solve_puzzle).grid(row=0, column=3)

        # canvas puzzle
        self.canvas_cells = []
        self.image_tiles = []
        self.current_state = goal_state

        self.grid_frame = tk.Frame(self.frame)
        self.grid_frame.grid(row=1, column=0, columnspan=4)



    # -------------------------------
    # Cargar imagen / dividirla
    # -------------------------------
    def load_image(self):
        path = filedialog.askopenfilename()
        if not path:
            return

        img = Image.open(path).convert("RGB")
        w, h = img.size
        tile_w = w // 3
        tile_h = h // 3

        self.image_tiles = []

        for r in range(3):
            for c in range(3):
                crop = img.crop((c * tile_w, r * tile_h, c * tile_w + tile_w, r * tile_h + tile_h))
                crop = crop.resize((120, 120))
                self.image_tiles.append(ImageTk.PhotoImage(crop))

        # preparar canvas
        for widget in self.grid_frame.winfo_children():
            widget.destroy()

        self.canvas_cells = []
        for i in range(9):
            cv = tk.Canvas(self.grid_frame, width=120, height=120, bg="white")
            cv.grid(row=i // 3, column=i % 3)
            self.canvas_cells.append(cv)

        self.current_state = goal_state
        self.draw_puzzle(self.current_state)

    def draw_puzzle(self, state):
        for i, tile in enumerate(state):
            canvas = self.canvas_cells[i]
            canvas.delete("all")
            if tile == 8 or not self.image_tiles:
                canvas.create_rectangle(0, 0, 120, 120, fill="white")
            else:
                canvas.create_image(0, 0, anchor="nw", image=self.image_tiles[tile])

    # -------------------------------
    # Barajar puzzle
    # -------------------------------
    def shuffle_puzzle(self):
        # Usamos un barajado moderado para aumentar la probabilidad
        # de que el estado estÃ© en el "espacio aprendido".
        self.current_state = shuffle_state(goal_state, moves_count=20)
        self.draw_puzzle(self.current_state)

    # -------------------------------
    # Entrenar Q-Learning
    # -------------------------------
    def train_agent(self):
        def run_training():
            steps, success = train_q_learning()

            # GrÃ¡fica de pasos por episodio
            plt.figure(figsize=(6, 3))
            plt.plot(steps)
            plt.title("Pasos por episodio")
            plt.grid()
            plt.tight_layout()
            plt.show()

            # GrÃ¡fica de Ã©xito por episodio
            plt.figure(figsize=(6, 3))
            plt.plot(success)
            plt.title("Ã‰xito por episodio (1=exito, 0=fracaso)")
            plt.grid()
            plt.tight_layout()
            plt.show()

        threading.Thread(target=run_training, daemon=True).start()

    # -------------------------------
    # Resolver puzzle con Q
    # -------------------------------
    def solve_puzzle(self):
        def animate():
            path = solve_with_Q_from_state(self.current_state, max_steps=200, epsilon_exec=0.05)

            for i, state in enumerate(path):
                self.draw_puzzle(state)

                print(f"\nðŸ”¹ Paso {i}")
                print("Estado actual:", state)
                print_transitions(state)

                # Mostrar acciÃ³n elegida segÃºn Q (si estÃ¡ entrenado)
                va = valid_actions(state)
                qvals = Q[state]
                best = max(va, key=lambda a: qvals[a])
                print(f"ðŸ‘‰ AcciÃ³n Ã³ptima segÃºn Q: {best}")

                time.sleep(0.3)

            self.current_state = path[-1]
            if self.current_state == goal_state:
                print("\nðŸŽ¯ Â¡Puzzle resuelto!\n")
            else:
                print("\nâš ï¸ No se pudo resolver completamente.\n")

        threading.Thread(target=animate, daemon=True).start()


def print_transitions(state):
    print(f"\n--- Transiciones desde estado {state} ---")
    for a in actions:
        new_s, r = step_env(state, a)
        print(f" AcciÃ³n {a:>5} â†’ {new_s}, Recompensa: {r}")
    print("--------------------------------------")



# ==================================================
# MAIN
# ==================================================

if __name__ == "__main__":
    root = tk.Tk()
    app = PuzzleApp(root)
    root.mainloop()
